{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b27b19b",
      "metadata": {
        "id": "2b27b19b"
      },
      "source": [
        "# Applied Analytics Portfolio\n",
        "\n",
        "**Predicting and Explaining Healthcare App Quality**\n",
        "\n",
        "Group: `_1_`\n",
        "\n",
        "Names & Student IDs: `Louis - `、`Christian - `、`Min Zhu - 5607778`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff1bb21",
      "metadata": {
        "id": "9ff1bb21"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Briefly describe the **decision context**:\n",
        "\n",
        "- Mental health unit that wants to recommend high-quality healthcare apps.\n",
        "- Patients have a range of mental illnesses and somatic comorbidities.\n",
        "\n",
        "Explain **why prediction helps** and what the **overall goal** of this portfolio is:\n",
        "\n",
        "- Use app metadata and user reviews to estimate whether an app is likely to be highly rated.\n",
        "- Identify key factors that drive user-perceived app quality.\n",
        "\n",
        "Conclude with a short **structure overview** of the notebook/report (what is done in Sections 2–5)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e775cfac",
      "metadata": {
        "id": "e775cfac"
      },
      "source": [
        "## 2. Data Understanding and Preparation\n",
        "\n",
        "### 2.1 Research Goal and Operationalization\n",
        "- Formulate a **precise prediction question**.\n",
        "- Define what **\"high-quality\" / \"highly rated\"** means (e.g., rating threshold + minimum number of ratings).\n",
        "- Specify which apps you will include (e.g., which categories, filters).\n",
        "\n",
        "### 2.2 Data Overview\n",
        "- Number of apps and reviews after filtering.\n",
        "- Brief description of key variables (metadata and text).\n",
        "\n",
        "### 2.3 Cleaning and Filtering\n",
        "- Handle outliers (e.g., extreme prices, extremely low number of ratings).\n",
        "- Check missing values in important variables and decide on imputation vs. dropping.\n",
        "- Document inclusion criteria and any comparator groups (e.g., medical vs. non-medical apps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0cfbbd",
      "metadata": {
        "id": "ac0cfbbd"
      },
      "outputs": [],
      "source": [
        "# 2. Data Understanding and Preparation\n",
        "# TODO: Load your datasets here and perform basic checks\n",
        "import pandas as pd\n",
        "\n",
        "# Example:\n",
        "# apps = pd.read_csv('apps.csv')\n",
        "# reviews = pd.read_csv('reviews.csv')\n",
        "# apps.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4917c936",
      "metadata": {
        "id": "4917c936"
      },
      "source": [
        "## 3. Data Exploration\n",
        "\n",
        "- Explore distributions of ratings, number of ratings, prices, categories, etc.\n",
        "- Visualize relevant relationships (e.g., rating vs. price, rating vs. category).\n",
        "- Use basic text mining on reviews: word frequencies, simple sentiment or topic structure.\n",
        "- Create and justify **new features** that may help prediction (e.g., sentiment score, review length, price bins).\n",
        "- Comment on what these patterns suggest about app quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bdb17c1",
      "metadata": {
        "id": "5bdb17c1"
      },
      "outputs": [],
      "source": [
        "# 3. Data Exploration\n",
        "# TODO: EDA plots and feature creation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example placeholder:\n",
        "# apps['log_ratings'] = np.log1p(apps['ratingCount'])\n",
        "# apps['averageRating'].hist()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9203dc",
      "metadata": {
        "id": "1d9203dc"
      },
      "source": [
        "## 4. Modeling Approach\n",
        "\n",
        "### 4.1 Review Sentiment with Zero-/Few-Shot Learning (SetFit or alternative)\n",
        "\n",
        "1. Define sentiment classes (e.g., positive / neutral / negative).\n",
        "2. Manually label a small, balanced subset of reviews.\n",
        "3. Fine-tune a SetFit model or a LLM model and evaluate performance.\n",
        "5. Aggregate predicted sentiment to the **app level** (e.g., share of positive reviews).\n",
        "\n",
        "These aggregated sentiment metrics will be used as features in Section 4.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdcb172",
      "metadata": {
        "id": "1bdcb172"
      },
      "outputs": [],
      "source": [
        "# 4.1 Sentiment Modeling\n",
        "# TODO: Implement SetFit / alternative sentiment classifier and aggregate results\n",
        "# Hint: start with a small labeled subset of reviews\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90adfa51",
      "metadata": {
        "id": "90adfa51"
      },
      "source": [
        "### 4.2 Predictive Modeling of App Quality\n",
        "\n",
        "1. **Define the target** variable at app level (e.g., high_quality = 1 if avg rating ≥ threshold and sufficient rating count).\n",
        "2. **Model A – Simple & interpretable:** Logistic Regression or a small Decision Tree.\n",
        "3. **Model B – More powerful:** e.g., Random Forest or Gradient Boosting with basic hyperparameter tuning.\n",
        "4. Compare performance (accuracy, precision, recall, F1, ROC-AUC, etc.) and comment on the trade-off between interpretability and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08be30d9",
      "metadata": {
        "id": "08be30d9"
      },
      "outputs": [],
      "source": [
        "# 4.2 Predictive Modeling of App Quality\n",
        "# TODO: Build train/test split, fit Model A and Model B, and evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Example placeholder:\n",
        "# X = apps_model_features\n",
        "# y = apps['high_quality']\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa0c1e2",
      "metadata": {
        "id": "7fa0c1e2"
      },
      "source": [
        "## 5. Interpretation and Argumentation of Results\n",
        "\n",
        "1. **Model Interpretation / Explainable AI**\n",
        "- Inspect and visualize feature importance (e.g., SHAP values or model-specific importances).\n",
        "- Discuss which features most strongly influence predicted app quality.\n",
        "\n",
        "2. **Fairness & Bias Reflection**\n",
        "- Where could sampling bias, measurement error, or missing data affect your results?\n",
        "- Briefly relate your reflections to fairness notions mentioned in the course.\n",
        "\n",
        "3. **LLM / SetFit as Method**\n",
        "- Discuss where these methods might introduce bias or instability.\n",
        "- Mention how sensitive your results are to label definitions or prompts (short reflection).\n",
        "\n",
        "4. **Practical Insights for the Clinic**\n",
        "- List 2–4 concrete, comprehensible recommendations that the mental health unit could use.\n",
        "- Focus on what your results *suggest they should pay attention to* when recommending apps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b4b4e0",
      "metadata": {
        "id": "91b4b4e0"
      },
      "source": [
        "## 6. AI Tools and References\n",
        "\n",
        "- Briefly describe where AI tools (e.g., ChatGPT, Copilot) were used, in line with FU guidelines.\n",
        "- List key papers, blog posts, or documentation that you relied on for methods.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
